{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Sid & Muskaan - PhaseI.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ4Xt1uf1LT7"
      },
      "source": [
        "# Final Project Phase 1 Summary\n",
        "This Jupyter Notebook (.ipynb) will serve as the skeleton file for your submission for Phase 1 of the Final Project. Answer all statements addressed below as specified in the instructions for the project, covering all necessary details. Please be clear and concise in your answers. Each response should be at most 3 sentences. Good luck! <br><br>\n",
        "\n",
        "Note: To edit a Markdown cell, double-click on its text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8OjIe2z1LT8"
      },
      "source": [
        "## About Your Topic\n",
        "<br>\n",
        "<b>Q1) What topic have you chosen for your final project?</b> <br>\n",
        "We will use the results of the 2016 election to answer questions and draw insights about a county's economy and overall health based on how the county voted in the 2016 election. <br> <br>\n",
        "<b>Q2) Why did you choose this specific topic and what are you looking to learn from the analysis? </b> <br>\n",
        "We chose this to understand how an areas political makeup correlates to their overall well-being as a society. From our analysis, we hope to draw correlations between poverty, unemployment and health based on how democratic or republican an area is. With the presidential election only a few days away, it's important to us to understand why certain communities vote the way they do. <br> <br>\n",
        "<b>Q3) Explain some of the concrete insights you expect to gather from your data and/or hypothesis you expect to answer. </b> <br>\n",
        "One insight we hope to see is whether primarily democratic or republican communities are prone to unemployment and educational deficits. Furthermore, we hope to draw coorelations between these ifgures and current coronavirus rates in each county to determine if any of the factors mentioned above contribute significantly to spread. We will use metadata such as income and disease rates to back our findings. We also hope to see how democratic and republican counties compare in regards to poverty levels, specifically underage poverty. <br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJkKO79R1LT9"
      },
      "source": [
        "## About Your Data\n",
        "<br>\n",
        "<b>Q4) Which dataset(s) and websites have you chosen to utilize for your analysis? Include the relevant filenames and links to your datasets. (Please follow the .ipynb hyperlink formatting for links, shown below.) </b> <br>\n",
        "\n",
        "Downloaded Data Set - CSV - this is incredibly detailed poverty information over the course of five years for counties throughout the US including every county in Georgia - the filename is ACSST5Y2015.S1701_data_with_overlays_2020-10-26T184714.csv, imported into this notebook as data.csv (rename the file). Be sure to download the 2015, 5 year zip folder. The downloaded data set is <a href = \"https://data.census.gov/cedsci/table?q=college%20poverty%20appling&g=0100000US.050000&tid=ACSST1Y2019.S1701&hidePreview=false\"> here.</a> \n",
        "\n",
        "Web Collection Requirement 1 - HTML - this is the election information for every county in GA for the last election, how many people voted Democrat and Republican, respectively. This information can be found <a href=\"https://en.wikipedia.org/wiki/2016_United_States_presidential_election_in_Georgia#By_county\">here.</a> \n",
        "\n",
        "Web Collection Requirement 2 - API - this dataset provides information about the past three election as well as numerous other demographic factors such as the county's demographics, education and school enrollment. This information can be found <a href = \"https://public.opendatasoft.com/api/records/1.0/search/?dataset=usa-2016-presidential-election-by-county&q=&rows=159&facet=state&facet=county&facet=precincts&refine.state=Georgia\"> here. </a>\n",
        "\n",
        "The extra dataset we used is an API that provides information about how much the Covid-19 pandemic has affected every county in Georgia. This information can be found <a href = \"https://services1.arcgis.com/bqfNVPUK3HOnCFmA/arcgis/rest/services/Coronavirus_Cases_in_Georgia/FeatureServer/0/query?where=1%3D1&outFields=OBJECTID,CountyName,NumberReportedCases,Deaths,CaseRate,Hospitalizations&outSR=4326&f=json\"> here. </a><br><br>\n",
        "<b>Q5) For the static file in your dataset collection, state the dimensions of the files in terms of row x columns and the file size (mb, gb, etc.) below. For example, 50,000 rows by 20 columns and 5.4 mb. If your file is a .json file, state the file size (mb, gb, etc.)</b><br>\n",
        "368 columns by 3223 rows and 7.5 mb. <br><br>\n",
        "<b>Q6) Explain why you chose these specific datasets and how they will be used in your analysis. </b><br>\n",
        "All of these data sets provide detailed information for every county in Georgia. The key dataset will be Web Collection Requirment 1 as it contains voter information for Georgia's counties. We will then cross this information with county demographics, Covid-19 effects, poverty and other factors which can be found through the other three data sets in order to identify specific correlations between these factors and the county's political leanings.\n",
        " <br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6K0cxaVA1LT-"
      },
      "source": [
        "## About Your Analysis\n",
        "<br>\n",
        "<b>Q7) Provide a list of steps of how you plan on performing your analysis and the way you will gather/present your findings. (Non-technical, high-level overview) </b><br>\n",
        "First we will gather the data from each of our respective sources, whether it be through HTML parsing, APIs or use downloaded data. Next, we will clean the data to remove anything that we do not need or any glaring flaws. Afterwards, we will take the clean data and use modules such as matplotlib, geoplotlib, Seaborn, etc, to visualize our data. We will finally use the visualized data to draw conclusions and present our findings. <br><br>\n",
        "<b>Q8) Explain how you intend on collecting, cleaning, and analyzing the data you gather as well as the manner in which you plan on presenting your insights. (More detailed, include modules, techniques, etc.). </b><br>\n",
        "We plan to use modules such as BeautifulSoup, requests and csv to collect our data. To clean our data, we will funnel all of our collected data into dictionaries, with the primary key being the county name, and the values being the data collected on each respective county. This is done rather efficiently using for loops to go through the data and pick out county names, values, etc.This will allow us to standardize and easily access data on a county. For example, county['forsyth']['%_dem'] will show us the percent that voted democratic in the 2016 election.  <br> <br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSiMishC1LT_"
      },
      "source": [
        "## About You\n",
        "<br>\n",
        "\n",
        "<b>Q9) List the name(s) of those working on this topic/project.</b><br>\n",
        "Siddharth Suman and Muskaan Vaishnav <br><br>\n",
        "<b> Please Initial Below that you acknowledge this statement: <br>\n",
        "I affirm that all of the work in this project will be done my me/my team and is not duplicated from any other source. In addition, any references that I use or code that I choose to model after will be appropriately credited and referenced in my project. </b><br>\n",
        "MV SS <br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fMkCDZe1LUA"
      },
      "source": [
        "## Your Questions (Optional)\n",
        "<br>\n",
        "Please add any clarifying questions that you would like answered below. Follow the same formatting as above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb9oVjpRDswQ"
      },
      "source": [
        "# Data Collection and Cleaning\n",
        "You are required to provide data collection and cleaning for the three (3) minimum datasets. Create a function for each of the following sections that reads or scrapes data from a file or website, manipulate and cleans the parsed data, and writes the cleaned data into a new file. \n",
        "\n",
        "Make sure your data cleaning and manipulation process is not too simple. Performing complex manipulation and using modules not taught in class shows effort, which will increase the chance of receiving full credit.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mRjxZDbE1tj"
      },
      "source": [
        "## Downloaded Dataset Requirement\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0p5xxmqzFGrO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fff28a18-c7fc-4e00-f2dd-6a23e5e2c9b0"
      },
      "source": [
        "import csv\n",
        "import json\n",
        "\n",
        "\n",
        "def data_parser(data):\n",
        "    with open(data) as f:\n",
        "        ''''reader = pd.read_csv(f, nrows=156, header = 1, skiprows = range(2,452))\n",
        "        # next(reader, None)  # skip the headers\n",
        "        dic = {}\n",
        "        for index, row in reader.iterrows():\n",
        "        d = {}\n",
        "        x = reader.to_dict('records')\n",
        "        print(reader)\n",
        "      '''\n",
        "        counties = {}\n",
        "        reader = csv.reader(f)\n",
        "        header = next(reader)\n",
        "        header = next(reader)[2:]\n",
        "        for row in reader:\n",
        "            if '0500000US13' in row[0]:\n",
        "                name = row[1]\n",
        "                find = name.index('County')\n",
        "                name = name[:find].strip()\n",
        "                counties[name.lower()] = {k: v for k, v in zip(header, row[2:])}\n",
        "        # print(counties['Forsyth']['Total!!Estimate!!AGE!!Under 18 years!!5 to 17 years'])\n",
        "        final = {}\n",
        "\n",
        "        for i in counties:\n",
        "            below_pov_lvl = float(\n",
        "                counties[i]['Percent below poverty level!!Estimate!!Population for whom poverty status is determined'])\n",
        "            u18_below_pv_lvl = float(counties[i]['Percent below poverty level!!Estimate!!AGE!!Under 18 years'])\n",
        "            non_hs_grads = 100 * (int(counties[i][\n",
        "                                          'Total!!Margin of Error!!EDUCATIONAL ATTAINMENT!!Population 25 years and over!!Less than high school graduate']) / int(\n",
        "                counties[i]['Total!!Estimate!!EDUCATIONAL ATTAINMENT!!Population 25 years and over']))\n",
        "            final[i] = {'Total % below poverty level': below_pov_lvl,\n",
        "                        '% below poverty level Under 18': u18_below_pv_lvl,\n",
        "                        '% less than highschool graduate': round(non_hs_grads, 1)}\n",
        "    with open('data_parser.json', 'w') as outfile:\n",
        "        json.dump(final, outfile, indent=2)\n",
        "    return print(len(final))\n",
        "    ############ Function Call ############\n",
        "\n",
        "\n",
        "data_parser('data.csv')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "159\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "794L4vGXFdYw"
      },
      "source": [
        "## Web Collection Requirement (HTML) \\#1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXwpJObDFiWM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ffc72881-b36d-4f48-9187-da0de6e5a81e"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import re\n",
        "def web_parser1():\n",
        "  data = requests.get('https://en.wikipedia.org/wiki/2016_United_States_presidential_election_in_Georgia#By_county')\n",
        "  soup = BeautifulSoup(data.text, 'html.parser')\n",
        "  tables = soup.find_all('tbody') #{'class':'odswidget-table__records-tbody'})\n",
        "  table = tables[14]\n",
        "  countylist = 'Appling Atkinson Bacon Baker Baldwin Banks Barrow Bartow Ben Hill Berrien Bibb Bleckley Brantley Brooks Bryan Bulloch Burke Butts Calhoun Camden Candler Carroll Catoosa Charlton Chatham Chattahoochee Chattooga Cherokee Clarke Clay\\\n",
        "  Clayton Clinch Cobb Coffee Colquitt Columbia Cook Coweta Crawford Crisp Dade Dawson DeKalb, Decatur Dodge Dooly Dougherty Douglas Early Echols Effingham Elbert Emanuel Evans Fannin Fayette Floyd Forsyth Franklin Fulton Gilmer Glascock, Glynn \\\n",
        "  Gordon Grady Greene Gwinnett Habersham Hall Hancock Haralson Harris Hart Heard Henry Houston Irwin Jackson Jasper Jeff Davis Jefferson Jenkins Johnson Jones Lamar Lanier Laurens Lee Liberty Lincoln Long Lowndes Lumpkin Macon Madison Marion McDuffie \\\n",
        "  McIntosh Meriwether Miller Mitchell Monroe Montgomery Morgan Murray Muscogee Newton Oconee Oglethorpe Paulding Peach Pickens Pierce Pike Polk Pulaski Putnam Quitman Rabun Randolph Richmond Rockdale Schley Screven Seminole Spalding Stephens Stewart Sumter Talbot \\\n",
        "  Taliaferro Tattnall Taylor Telfair Terrell Thomas Tift Toombs Towns Treutlen Troup Turner Twiggs Union Upson Walker Walton Ware Warren Washington Wayne Webster Wheeler White Whitfield Wilcox Wilkes Wilkinson Worth'\n",
        "  final = {}\n",
        "  info = table.text\n",
        "  info = info.strip().split()\n",
        "  for county in info[8:]:\n",
        "      cdict = {}\n",
        "      cdict = {}\n",
        "      if info.index(county) != 1281:\n",
        "        if info[info.index(county) + 1] in countylist and county in countylist and info.index(county) != 1280:\n",
        "            cdict['Clinton %'] = float(re.sub('%','',info[info.index(county) + 2]))\n",
        "            cdict['Clinton Votes'] = int(re.sub(',','',info[info.index(county) + 3]))\n",
        "            cdict['Trump %'] = float(re.sub('%','',info[info.index(county) + 4]))\n",
        "            cdict['Trump Votes'] = int(re.sub(',','',info[info.index(county) + 5]))\n",
        "            cdict['Others %'] = float(re.sub('%','',info[info.index(county) + 6]))\n",
        "            cdict['Other Votes'] = int(re.sub(',','',info[info.index(county) + 7]))\n",
        "            cdict['Total Votes'] = int(re.sub(',','',info[info.index(county) + 8]))\n",
        "            final[county.lower() + ' ' +info[info.index(county) + 1].lower()] = cdict\n",
        "      if county[0] in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' and (county in countylist) and county not in ['Jeff', 'Davis', 'Ben', 'Hill']:\n",
        "          cdict['Clinton %'] = float(re.sub('%','',info[info.index(county) + 1]))\n",
        "          cdict['Clinton Votes'] = int(re.sub(',','',info[info.index(county) + 2]))\n",
        "          cdict['Trump %'] = float(re.sub('%','',info[info.index(county) + 3]))\n",
        "          cdict['Trump Votes'] = int(re.sub(',','',info[info.index(county) + 4]))\n",
        "          cdict['Others %'] = float(re.sub('%','',info[info.index(county) + 5]))\n",
        "          cdict['Other Votes'] = int(re.sub(',','',info[info.index(county) + 6]))\n",
        "          cdict['Total Votes'] = int(re.sub(',','',info[info.index(county) + 7]))\n",
        "          final[county.lower()] = cdict \n",
        "  with open('html_parser.json', 'w') as outfile:\n",
        "    json.dump(final, outfile, indent = 2)\n",
        "  return len(final)  \n",
        "\n",
        "\n",
        "web_parser1()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "159"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gREHpeZD7hkB"
      },
      "source": [
        "## Web Collection Requirement (API)\\#2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzgMg9ji7hkE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d33fd447-6001-4adf-b626-07b7d55882bf"
      },
      "source": [
        "import requests, json\n",
        "import pandas as pd\n",
        "\n",
        "def web_parser2():\n",
        "    api = requests.get('https://public.opendatasoft.com/api/records/1.0/search/?dataset=usa-2016-presidential-election-by-county&q=&rows=159&facet=state&facet=county&facet=precincts&refine.state=Georgia').content\n",
        "    data = json.loads(api)\n",
        "    #county = {}\n",
        "    df = pd.json_normalize(data['records'])\n",
        "    df = df.set_index('fields.name_16').sort_index()\n",
        "    cleaned_df = df.dropna(axis=1)\n",
        "    uppercase = cleaned_df.to_dict(orient='index')\n",
        "    county = dict((k.lower(), v) for k, v in uppercase.items())\n",
        "    for i in county:\n",
        "        try:\n",
        "            del county[i]['datasetid']\n",
        "            del county[i]['recordid']\n",
        "            del county[i]['record_timestamp']\n",
        "        except:\n",
        "            continue\n",
        "    '''for i in data['records']:\n",
        "        county[i['fields']['name_16'].lower()]=i['fields']\n",
        "    for i in county:\n",
        "        try:\n",
        "            del county[i]['geo_shape']\n",
        "            del county[i]['temp_bins']\n",
        "        except:\n",
        "            continue'''\n",
        "    with open('web_parser2.json', 'w') as outfile:\n",
        "        json.dump(county, outfile, indent=2)\n",
        "    return len(county)\n",
        "############ Function Call ############\n",
        "web_parser2()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "159"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDD6sMsCXRxc"
      },
      "source": [
        "## Web Collection Requirement *Additional* (API) \\#3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAkUOqMgXQJG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f978cc65-f769-4754-f864-c8699a074027"
      },
      "source": [
        "import requests\n",
        "import json\n",
        "def web_parser3():\n",
        "  data = requests.get('https://services1.arcgis.com/bqfNVPUK3HOnCFmA/arcgis/rest/services/Coronavirus_Cases_in_Georgia/FeatureServer/0/query?where=1%3D1&outFields=OBJECTID,CountyName,NumberReportedCases,Deaths,CaseRate,Hospitalizations&outSR=4326&f=json')\n",
        "  covid = data.json()\n",
        "  countylist = 'Appling Atkinson Bacon Bartow Baker Baldwin Banks Barrow Ben Hill Berrien Bibb Bleckley Brantley Brooks Bryan Bulloch Burke Butts Calhoun Camden Candler Carroll Catoosa Charlton Chatham Chattahoochee Chattooga Cherokee Clarke Clay\\\n",
        "  Clayton Clinch Cobb Coffee Colquitt Columbia Cook Coweta Crawford Crisp Dade Dawson DeKalb, Decatur Dodge Dooly Dougherty Douglas Early Echols Effingham Elbert Emanuel Evans Fannin Fayette Floyd Forsyth Franklin Fulton Gilmer Glascock, Glynn \\\n",
        "  Gordon Grady Greene Gwinnett Habersham Hall Hancock Haralson Harris Hart Heard Henry Houston Irwin Jackson Jasper Jeff Davis Jefferson Jenkins Johnson Jones Lamar Lanier Laurens Lee Liberty Lincoln Long Lowndes Lumpkin Macon Madison Marion McDuffie \\\n",
        "  McIntosh Meriwether Miller Mitchell Monroe Montgomery Morgan Murray Muscogee Newton Oconee Oglethorpe Paulding Peach Pickens Pierce Pike Polk Pulaski Putnam Quitman Rabun Randolph Richmond Rockdale Schley Screven Seminole Spalding Stephens Stewart Sumter Talbot \\\n",
        "  Taliaferro Tattnall Taylor Telfair Terrell Thomas Tift Toombs Towns Treutlen Troup Turner Twiggs Union Upson Walker Walton Ware Warren Washington Wayne Webster Wheeler White Whitfield Wilcox Wilkes Wilkinson Worth'\n",
        "  final = {}\n",
        "  counties = covid['features']\n",
        "  for dic in counties:\n",
        "    subfinal = {}\n",
        "    county = dic['attributes']\n",
        "    if county['CountyName'] in countylist:\n",
        "      subfinal['Reported Cases'] = county['NumberReportedCases']\n",
        "      subfinal['Reported Deaths'] = county['Deaths']\n",
        "      subfinal['Case Rate'] = county['CaseRate']\n",
        "      subfinal['Hospitalizations'] = county['Hospitalizations']\n",
        "      final[county['CountyName'].strip().lower()] = subfinal\n",
        "  with open('web_parser3additional.json', 'w') as outfile:\n",
        "        json.dump(final, outfile, indent=2)\n",
        "  return len(final)\n",
        "\n",
        "############ Function Call ############\n",
        "web_parser3()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "159"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6qTMw1a7hD4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uttEYrm9US5s"
      },
      "source": [
        "#Inconsistencies\n",
        "For each inconsistency (NaN, null, duplicate values, empty strings, etc.) you discover in your datasets, write at least 2 sentences stating the significance, how you identified it, and how you handled it.\n",
        "\n",
        "1. In the downloaded data, one subtle but major inconsistency was the fact that Appling County, GA row was repeated twice in the data, once where it should be and again at the end. When I was originally collecting the data into a pandas dataframe, I kept getting 2 Appling County rows. Only after realzing that there were two rows, did I switch to using dictionaries. Because there can't be two keys with the same name in a dictionary and since both rows had the same data, the problem was solved.\n",
        "\n",
        "2. In the Web Collection #2 dataset, the 2016 election API, an inconsistency was that some counties had a key for their geographic shape and coordinates, while others did not. \n",
        "For example the code below would result in a key error.\n",
        "```\n",
        "county['forsyth']['geo_shape']\n",
        "```\n",
        "However, other code such as the one below would return geographic coordinates and a shape.\n",
        "```\n",
        "county['evans']['geo_shape']\n",
        "```\n",
        "To remedy this, I first converted my data into a pandas dataframe. Next, I used the dropna() function in pandas to drop any columns that contained NaN. Finally, I converted the dataframe to a dictionary using to_dict() with the county name as the key.\n",
        "\n",
        "\n",
        "```\n",
        "    df = pd.json_normalize(data['records'])\n",
        "    df = df.set_index('fields.name_16')\n",
        "    cleaned_df = df.dropna(axis=1)\n",
        "    uppercase = cleaned_df.to_dict(orient='index')\n",
        "    county = dict((k.lower(), v) for k, v in uppercase.items())\n",
        "```\n",
        "\n",
        "\n",
        "3. In the Web Collection #1 dataset, when the code was parsed through and converted to dictionaries, I realized that the code was returning 169 counties instead of h=the 159 that exist in the county. Remedying this was a two-step process. The first thing that I did was I found a list of every county in Georgia and ensured that there were no counties being added to the dictionary that were not counties in Georgia. I executed this through writing and if statement. This left me with 161 counties instead of the desired 159. After a thorough search of the code and its output, I realized that the way I had written my code accounted for Jeff Davis county and Ben Hill county twice, once under the first name and a second time under the  second name. To account for this descrepancy, I created a separate conditional designed specifically for counties with 2 names and was able to add it to the dictionary properly in this way.\n",
        "\n",
        "4. In the Web Collection #4 dataset, I found that I was having a similar issue as in the Web Collection #1 dataset. It was including 2 counties that were not Georgia counties. Because the issue was so similar to one I had seen before, I was able to use the same method to resolve it. I used the list of Georgia counties that I had found earlier and was able to create a conditional checking to see if the county was in fact a county within Georgia. Through doing this, I removed all counties that did not exist in Georgia and created a clean dictionary this way. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Dp7Pm-Suh3d"
      },
      "source": [
        "## Data Sources\n",
        "Include sources (as links) to your datasets.\n",
        "*   Downloaded Dataset Source(**CSV**): https://data.census.gov/cedsci/table?q=college%20poverty%20appling&g=0100000US.050000&tid=ACSST1Y2019.S1701&hidePreview=false\n",
        "*   Web Collection #1 Source(**HTML**): https://en.wikipedia.org/wiki/2016_United_States_presidential_election_in_Georgia\n",
        "*   Web Collection #2 Source(**API**): https://public.opendatasoft.com/explore/dataset/usa-2016-presidential-election-by-county/table/?disjunctive.state&refine.state=Georgia&dataChart=eyJxdWVyaWVzIjpbeyJjaGFydHMiOlt7InR5cGUiOiJjb2x1bW4iLCJmdW5jIjoiQVZHIiwieUF4aXMiOiJyZXAxNl9mcmFjIiwic2NpZW50aWZpY0Rpc3BsYXkiOnRydWUsImNvbG9yIjoiI0U5MUQwRSJ9LHsidHlwZSI6ImNvbHVtbiIsImZ1bmMiOiJBVkciLCJ5QXhpcyI6ImRlbTE2X2ZyYWMiLCJzY2llbnRpZmljRGlzcGxheSI6dHJ1ZSwiY29sb3IiOiIjMjMyMDY2In1dLCJ4QXhpcyI6InN0YXRlIiwibWF4cG9pbnRzIjoyMDAsInNvcnQiOiIiLCJjb25maWciOnsiZGF0YXNldCI6InVzYS0yMDE2LXByZXNpZGVudGlhbC1lbGVjdGlvbi1ieS1jb3VudHkiLCJvcHRpb25zIjp7ImRpc2p1bmN0aXZlLnN0YXRlIjp0cnVlLCJyZWZpbmUuc3RhdGUiOiJHZW9yZ2lhIn19fV0sInRpbWVzY2FsZSI6IiIsImRpc3BsYXlMZWdlbmQiOnRydWUsImFsaWduTW9udGgiOnRydWV9\n",
        "*   Web Collection #3 Source **Additional** (**API**): https://opendata.atlantaregional.com/datasets/f239d3d2d9874cda9d75cf652158068c_0/geoservice\n",
        "\n",
        "\n"
      ]
    }
  ]
}